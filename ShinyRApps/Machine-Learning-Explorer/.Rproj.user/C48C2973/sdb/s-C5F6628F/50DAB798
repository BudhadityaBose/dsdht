{
    "contents" : "######################################################\n######################################################\n#####[04] Multiple Linear Regression Lecture Code#####\n######################################################\n######################################################\n\n\n\n#####################################################\n#####Example using the State Information Dataset#####\n#####################################################\nhelp(state.x77)\nstate.x77 #Investigating the state.x77 dataset.\n\nstates = as.data.frame(state.x77) #Forcing the state.x77 dataset to be a dataframe.\n\n#Cleaning up the column names so that there are no spaces.\ncolnames(states)[4] = \"Life.Exp\"\ncolnames(states)[6] = \"HS.Grad\"\n\n#Creating a population density variable.\nstates[,9] = (states$Population*1000)/states$Area\ncolnames(states)[9] = \"Density\"\n\n#Basic numerical EDA for states dataset.\nsummary(states)\nsapply(states, sd)\ncor(states)\n\n#Basic graphical EDA for the states dataset.\nplot(states)\n\n#Can we estimate an individual's life expectancy based upon the state in which\n#they reside?\n\n#Creating a saturated model (a model with all variables included).\nmodel.saturated = lm(Life.Exp ~ ., data = states)\n\n##################TEST\n#adjust on highest Adjusted R-squared\n#lowest AIC\nst=states\nvariables = names(st)\nindependent = \"Life.Exp\"\neval(parse(text=paste(\"st$\",independent,\" = 1\",sep=\"\")))\n\nx <- st$Life.Exp\n  for (i in 1:ncol(st)){\n        y <- st[eval(colnames(st)[i])][,1]\n        aovr <- summary(lm(x ~ y)) #Conducting the One-Way ANOVA on the weight\n        Pr_of_y = aovr$coefficients[,\"Pr(>|t|)\"][\"y\"]\n      if (Pr_of_y<0.025){ \n        print(paste(\"  SIGNIFICANT Name:\",\n          colnames(st)[i],\" Adjr2:\",aovr$adj.r.squared,\"Pr(>|t|):\",Pr_of_y))\n      }else{\n        print(paste(\"INSIGNIFICANT Name:\",\n          colnames(st)[i],\" Adjr2:\",aovr$adj.r.squared,\"Pr(>|t|):\",Pr_of_y))\n      }\n  }\n##################TEST\n\nsummary(model.saturated) #Many predictor variables are not significant, yet the\n                         #overall regression is significant.\n\nplot(model.saturated) #Assessing the assumptions of the model.\n\nlibrary(car) #Companion to applied regression.\ninfluencePlot(model.saturated)\n\nvif(model.saturated) #Assessing the variance inflation factors for the variables\n                     #in our model.\n\n#Added variable plots for assessing the contribution of each additional variable.\navPlots(model.saturated) #Distinct patterns are indications of good contributions\n                         #to the model; absent patterns usually are pointers to\n                         #variables that could be dropped from the model.\n\n#We note that Illiteracy has a large VIF, an insignificant p-value in the overall\n#regression, and no strong distinct pattern in the added-variable plot. What\n#happens when we remove it from the model?\nmodel2 = lm(Life.Exp ~ . - Illiteracy, data = states)\n\nsummary(model2) #R^2 adjusted went up, model still significant, etc.\nsummary(model.saturated)\n\nplot(model2) #No overt additional violations.\n\ninfluencePlot(model2) #No overt additional violations; Hawaii actually lowers\n                      #its hat value (leverage).\n\nvif(model2) #VIFs all decrease.\n\n#We can compare these two models using a partial F-test using the anova function.\n#Here, the first model we supply is the reduced model, and the second is the full\n#model.\nanova(model2, model.saturated) #The p-value is quite large, indicating that we\n                               #retain the null hypothesis. Recall that the null\n                               #hypothesis is that the slope coefficients of the\n                               #variables in the subset of interest are all 0.\n                               #We retain this hypothesis and conclude that the\n                               #Illiteracy variable is not informative in our\n                               #model; we move forward with the reduced model.\n\n#Let's use the partial F-test to test multiple predictors at once. As compared\n#to the saturated model, does the subset of Illiteracy, Area, and Income have\n#any effect on our prediction of Life.Exp?\nmodel.full = lm(Life.Exp ~ ., data = states)\nmodel.reduced = lm(Life.Exp ~ . - Illiteracy - Area - Income, data = states)\n\nanova(model.reduced, model.full) #The p-value is quite large; thus, the reduced\n                                 #model is sufficient.\n\n#Checking the model summary and assumptions of the reduced model.\nsummary(model.reduced)\nplot(model.reduced)\ninfluencePlot(model.reduced)\nvif(model.reduced)\n\n#We can also inspect the AIC and BIC values to compare various models.\nAIC(model.full,    #Model with all variables.\n    model2,        #Model with all variables EXCEPT Illiteracy.\n    model.reduced) #Model with all variables EXCEPT Illiteracy, Area, and Income.\n\nBIC(model.full,\n    model2,\n    model.reduced) #Both the minimum AIC and BIC values appear alongside the\n                   #reduced model that we tested above.\n\n#We can use stepwise regression to help automate the variable selection process.\n#Here we define the minimal model, the full model, and the scope of the models\n#through which to search:\nmodel.empty = lm(Life.Exp ~ 1, data = states) #The model with an intercept ONLY.\nmodel.full = lm(Life.Exp ~ ., data = states) #The model with ALL variables.\nscope = list(lower = formula(model.empty), upper = formula(model.full))\n\nlibrary(MASS) #The Modern Applied Statistics library.\n\n#Stepwise regression using AIC as the criteria (the penalty k = 2).\nforwardAIC = step(model.empty, scope, direction = \"forward\", k = 2)\nbackwardAIC = step(model.full, scope, direction = \"backward\", k = 2)\nbothAIC.empty = step(model.empty, scope, direction = \"both\", k = 2)\nbothAIC.full = step(model.full, scope, direction = \"both\", k = 2)\n\n#Stepwise regression using BIC as the criteria (the penalty k = log(n)).\nforwardBIC = step(model.empty, scope, direction = \"forward\", k = log(50))\nbackwardBIC = step(model.full, scope, direction = \"backward\", k = log(50))\nbothBIC.empty = step(model.empty, scope, direction = \"both\", k = log(50))\nbothBIC.full = step(model.full, scope, direction = \"both\", k = log(50))\n\n#In this case, all procedures yield the model with only the Murder, HS.Grad,\n#Frost, and Population variables intact.\n\n#Checking the model summary and assumptions of the reduced model.\nsummary(forwardAIC)\nplot(forwardAIC)\ninfluencePlot(forwardAIC)\nvif(forwardAIC)\navPlots(forwardAIC)\nconfint(forwardAIC)\n\n#Predicting new observations.\nforwardAIC$fitted.values #Returns the fitted values.\n\nnewdata = data.frame(Murder = c(1.5, 7.5, 12.5),\n                     HS.Grad = c(60, 50, 40),\n                     Frost = c(75, 55, 175),\n                     Population = c(7500, 554, 1212))\n\npredict(forwardAIC, newdata, interval = \"confidence\") #Construct confidence intervals\n                                                      #for the average value of an\n                                                      #outcome at a specific point.\n\npredict(forwardAIC, newdata, interval = \"prediction\") #Construct prediction invervals\n                                                      #for a single observation's\n                                                      #outcome value at a specific point.\n\n\n\n#####################################\n#####Extending Model Flexibility#####\n#####################################\ntests = read.table(\"04testscores.txt\")\n\n#Basic numerical EDA for states dataset.\nsummary(tests)\nsd(tests$Test.Score)\nsd(tests$Hours.Studied)\ncor(tests$Test.Score, tests$Hours.Studied)\n\n#Basic graphical EDA for tests dataset.\nplot(tests$Hours.Studied, tests$Test.Score)\n\n#############################################\n#####Fitting a simple linear regression.#####\n#############################################\nmodel.simple = lm(Test.Score ~ Hours.Studied, data = tests)\n\nsummary(model.simple) #Investigating the model and assessing some diagnostics.\nplot(model.simple)\ninfluencePlot(model.simple)\n\n#Constructing confidence and prediction bands for the scope of our data.\nnewdata = data.frame(Hours.Studied = seq(1, 3, length = 100))\nconf.band = predict(model.simple, newdata, interval = \"confidence\")\npred.band = predict(model.simple, newdata, interval = \"prediction\")\n\nplot(tests$Hours.Studied, tests$Test.Score,\n     xlab = \"Hours Studied\", ylab = \"Test Score\",\n     main = \"Simple Linear Regression Model\\nTests Dataset\")\nabline(model.simple, lty = 2)\nlines(newdata$Hours.Studied, conf.band[, 2], col = \"blue\") #Plotting the lower confidence band.\nlines(newdata$Hours.Studied, conf.band[, 3], col = \"blue\") #Plotting the upper confidence band.\nlines(newdata$Hours.Studied, pred.band[, 2], col = \"red\") #Plotting the lower prediction band.\nlines(newdata$Hours.Studied, pred.band[, 3], col = \"red\") #Plotting the upper prediction band.\nlegend(\"topleft\", c(\"Regression Line\", \"Conf. Band\", \"Pred. Band\"),\n       lty = c(2, 1, 1), col = c(\"black\", \"blue\", \"red\"))\n\n##################################\n#####Adding a quadratic term.#####\n##################################\nmodel.quadratic = lm(Test.Score ~ Hours.Studied + I(Hours.Studied^2), data = tests)\n\nsummary(model.quadratic) #Investigating the model and assessing some diagnostics.\nplot(model.quadratic)\ninfluencePlot(model.quadratic)\n\n#Constructing confidence and prediction bands for the scope of our data.\nconf.band = predict(model.quadratic, newdata, interval = \"confidence\")\npred.band = predict(model.quadratic, newdata, interval = \"prediction\")\n\nplot(tests$Hours.Studied, tests$Test.Score,\n     xlab = \"Hours Studied\", ylab = \"Test Score\",\n     main = \"Quadratic Regression Model\\nTests Dataset\")\nlines(tests$Hours.Studied[order(tests$Hours.Studied)],\n      model.quadratic$fitted.values[order(tests$Hours.Studied)], lty = 2)\nlines(newdata$Hours.Studied, conf.band[, 2], col = \"blue\") #Plotting the lower confidence band.\nlines(newdata$Hours.Studied, conf.band[, 3], col = \"blue\") #Plotting the upper confidence band.\nlines(newdata$Hours.Studied, pred.band[, 2], col = \"red\") #Plotting the lower prediction band.\nlines(newdata$Hours.Studied, pred.band[, 3], col = \"red\") #Plotting the upper prediction band.\nlegend(\"topleft\", c(\"Regression Line\", \"Conf. Band\", \"Pred. Band\"),\n       lty = c(2, 1, 1), col = c(\"black\", \"blue\", \"red\"))\n\n##########################\n#####Adding a factor.#####\n##########################\nmodel.factor = lm(Test.Score ~ Hours.Studied + Gender, data = tests)\n\nsummary(model.factor) #Investigating the model and assessing some diagnostics.\nplot(model.factor)\ninfluencePlot(model.factor)\n\ncol.vec = c(rep(\"pink\", 250), rep(\"blue\", 250))\n\nplot(tests$Hours.Studied, tests$Test.Score, col = col.vec,\n     xlab = \"Hours Studied\", ylab = \"Test Score\",\n     main = \"Linear Regression Model w/ Factor\\nTests Dataset\")\nabline(model.factor$coefficients[1], #Intercept for females.\n       model.factor$coefficients[2], #Slope for females.\n       lwd = 3, lty = 2, col = \"pink\")\nabline(model.factor$coefficients[1] + model.factor$coefficients[3], #Intercept for males.\n       model.factor$coefficients[2], #Slope for males.\n       lwd = 3, lty = 2, col = \"blue\")\nlegend(\"topleft\", c(\"Female Regression\", \"Male Regression\"),\n       lwd = 3, lty = 2, col = c(\"pink\", \"blue\"))",
    "created" : 1444320603284.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2281100720",
    "id" : "50DAB798",
    "lastKnownWriteTime" : 1444404560,
    "path" : "~/boxer/data/Week3LectureCodeData/04 Multiple Linear Regression Lecture Code.R",
    "project_path" : "data/Week3LectureCodeData/04 Multiple Linear Regression Lecture Code.R",
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "type" : "r_source"
}