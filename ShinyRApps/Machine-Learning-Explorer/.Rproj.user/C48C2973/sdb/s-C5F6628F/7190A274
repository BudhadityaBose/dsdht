{
    "contents" : "####################################################\n####################################################\n#####[03] Simple Linear Regression Lecture Code#####\n####################################################\n####################################################\n\n\n\n##############################################\n#####Manual example with the cars dataset#####\n##############################################\nhelp(cars)\ncars #Investigating the cars dataset.\n\n#Basic numerical EDA for cars dataset.\nsummary(cars) #Five number summaries.\nsapply(cars, sd) #Standard deviations.\ncor(cars) #Correlations.\n\n#Basic graphical EDA for cars dataset.\nhist(cars$speed, xlab = \"Speed in MPH\", main = \"Histogram of Speed\")\nhist(cars$dist, xlab = \"Distance in Feet\", main = \"Histogram of Distance\")\n\nplot(cars, xlab = \"Speed in MPH\", ylab = \"Distance in Feet\",\n     main = \"Scatterplot of Cars Dataset\")\n\n#Manual calculation of simple linear regression coefficients.\nbeta1 = sum((cars$speed - mean(cars$speed)) * (cars$dist - mean(cars$dist))) /\n  sum((cars$speed - mean(cars$speed))^2)\n\nbeta0 = mean(cars$dist) - beta1*mean(cars$speed)\n\n#Adding the least squares regression line to the plot.\nabline(beta0, beta1, lty = 2)\n\n#Calculating the residual values.\nresiduals = cars$dist - (beta0 + beta1*cars$speed)\n\n#Note the sum of the residuals is 0.\nsum(residuals)\n\n#Visualizing the residuals.\nsegments(cars$speed, cars$dist,\n         cars$speed, (beta0 + beta1*cars$speed),\n         col = \"red\")\ntext(cars$speed - .5, cars$dist, round(residuals, 2), cex = 0.5)\n\n\n\n#################################################\n#####Automatic example with the cars dataset#####\n#################################################\nmodel = lm(dist ~ speed, data = cars) #Use the linear model function lm() to\n                                      #conduct the simple linear regression.\n\nsummary(model) #All the summary information for the model in question. Reports:\n               #-The five number summary of the residuals.\n               #-The coefficient estimates.\n               #-The coeffiient standard errors.\n               #-The t-test for significance of the coefficient estimates.\n               #-The p-values for the significance tests.\n               #-The level of significance.\n               #-The RSE and degrees of freedom for the model.\n               #-The coefficient of determination, R^2.\n               #-The overall model F-statistic and corresponding p-value.\n\n#The equation of the model can be constructed from the output:\n#Predicted Distance = -17.6 + (3.9)*Speed\n\n#The interpretation for the slope coefficient: With a 1 MPH increase in car speed,\n#the stopping distance, on average, increases by approximately 3.9 feet.\n\n#The interpretation for the intercept coefficient: When a car's speed is 0 MPH,\n#the stopping distance, on average, is -17.6 MPH. Theoretically, does this make\n#sense? Why might this be the case?\n\n#The residual standard error is about 15.38; this is an approximation of how much\n#the residuas tend to deviate around the regression line.\n\n#The coefficient of determination is about 0.65; approximately 65% of the variability\n#in the distance variable is explained by the speed variable.\n\n#The intercept, slope, and overall regression is extremely significant (p-values\n#all below 0.05).\n\n#Notice that the F-statistic value for the overall regression is the same as the\n#square of the t-statistic value for the speed coefficient:\nt.statistic = 9.464\nf.statistic = 89.57\nt.statistic^2\n\nconfint(model) #Creating 95% confidence intervals for the model coefficients.\n\n\n\n####################################################\n#####Checking assumptions with the cars dataset#####\n####################################################\n#Linearity\nplot(cars, xlab = \"Speed in MPH\", ylab = \"Distance in Feet\",\n     main = \"Scatterplot of Cars Dataset\")\nabline(model, lty = 2)\n\n#Constant Variance & Independent Errors\nplot(model$fitted, model$residuals,\n     xlab = \"Fitted Values\", ylab = \"Residual Values\",\n     main = \"Residual Plot for Cars Dataset\")\nabline(h = 0, lty = 2)\n\n#Normality\nqqnorm(model$residuals)\nqqline(model$residuals)\n\n#Using the built-in plot() function to visualize the residual plots.\nplot(model) #Note the addition of the loess smoother and scale-location plot\n            #in order to assess whether there is a pronounced non-linear\n            #relationship. Also the addition of leverage and cook's distance.\n\n#Outliers are observations that have high residual values. The error for these\n#observations is relatively large because the observations fall distant from the\n#regression line.\n\n#Leverage points are observations that have unusually small or large independent\n#variable values; these points fall far from the mean. Thus, these observations\n#have a lot of leverage to change the slope of the regression line. The further\n#an observation is from the mean of the independent variable, the more leverage\n#it has on the slope.\n\n#Cook's distance helps to measure the effect of deleting an observation from the\n#dataset and rerunning the regression. Observations that have large residual values\n#and also high leverage tend to pose threats to the accuracy of the regression\n#line and thus need to be further investigated.\n\n#Visualizing another influence plot for the regression model.\nlibrary(car) #Companion to applied regression.\ninfluencePlot(model)\n\n#####################################\n#####Predicting New Observations#####\n#####################################\nmodel$fitted.values #Returns the fitted values.\n\nnewdata = data.frame(speed = c(15, 20, 25)) #Creating a new data frame to pass\n                                            #to the predict() function.\n\npredict(model, newdata, interval = \"confidence\") #Construct confidence intervals\n                                                 #for the average value of an\n                                                 #outcome at a specific point.\n\npredict(model, newdata, interval = \"prediction\") #Construct prediction invervals\n                                                 #for a single observation's\n                                                 #outcome value at a specific point.\n\n#Constructing confidence and prediction bands for the scope of our data.\nnewdata = data.frame(speed = 4:25)\nconf.band = predict(model, newdata, interval = \"confidence\")\npred.band = predict(model, newdata, interval = \"prediction\")\n\n#Visualizing the confidence and prediction bands\nplot(cars, xlab = \"Speed in MPH\", ylab = \"Distance in Feet\",\n     main = \"Scatterplot of Cars Dataset\")\nabline(model, lty = 2) #Plotting the regression line.\nlines(newdata$speed, conf.band[, 2], col = \"blue\") #Plotting the lower confidence band.\nlines(newdata$speed, conf.band[, 3], col = \"blue\") #Plotting the upper confidence band.\nlines(newdata$speed, pred.band[, 2], col = \"red\") #Plotting the lower prediction band.\nlines(newdata$speed, pred.band[, 3], col = \"red\") #Plotting the upper prediction band.\nlegend(\"topleft\", c(\"Regression Line\", \"Conf. Band\", \"Pred. Band\"),\n       lty = c(2, 1, 1), col = c(\"black\", \"blue\", \"red\"))\n\n\n\n####################################\n#####The Box-Cox Transformation#####\n####################################\nbc = boxCox(model) #Automatically plots a 95% confidence interval for the lambda\n                   #value that maximizes the likelihhood of transforming to\n                   #normality.\n\nlambda = bc$x[which(bc$y == max(bc$y))] #Extracting the best lambda value.\n\ndist.bc = (cars$dist^lambda - 1)/lambda #Applying the Box-Cox transformation.\n\nmodel.bc = lm(dist.bc ~ cars$speed) #Creating a new regression based on the\n                                    #transformed variable.\n\nsummary(model.bc) #Assessing the output of the new model.\n\nplot(model.bc) #Assessing the assumptions of the new model.\n\nboxCox(model.bc) #What happens if we want to apply the Box-Cox transformation\n                 #a second time?",
    "created" : 1444320556067.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3300272886",
    "id" : "7190A274",
    "lastKnownWriteTime" : 1444235475,
    "path" : "~/boxer/data/Week3LectureCodeData/03 Simple Linear Regression Lecture Code.R",
    "project_path" : "data/Week3LectureCodeData/03 Simple Linear Regression Lecture Code.R",
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "type" : "r_source"
}